<!DOCTYPE html>
<html lang="en">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="UTF-8">
  <title>Formalization of Trajectory Space Search - Niel Ok</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="../site.css">
</head>
<body>
  <a href="javascript:window.history.back()">Back</a>

  <p></p>
    <!-- Entries -->
  <div class="entry", id="entry-2025-05-02">
    <div class="entry-date">May 2, 2025</div>
    <div class="entry-category">trajectory space search</div>
    <h2>Formalization of Trajectory Space Search</h2>
    <div class="entry-content">
      This post formalizes the theory I have been calling trajectory space search in stochastic diffusion models. The key idea is to shift the search domain from noise space to trajectory space as we move from deterministic diffusers (under DeepMind's framework) to stochastic ones, potentially enabling more expressive inference.
  
      <p><strong>1. Definitions</strong></p>
      <ul>
        <li>\( \mathcal{X} \): data space (e.g., images, text sequences)</li>
        <li>\( x_T \sim p_T(x) \): initial noise drawn from a base Gaussian distribution</li>
        <li>\( x_t \): sample at diffusion step \( t \) (from \( T \) to 0)</li>
        <li>\( \pi = \{x_T, x_{T-1}, \ldots, x_0\} \): a full denoising trajectory</li>
        <li>\( \pi(x_T) \): a trajectory sampled from the default reverse process starting at \( x_T \)</li>
        <li>\( \mathcal{T}(x_T) \): set of all possible trajectories starting at \( x_T \)</li>
        <li>\( f(\pi) \): objective function scoring the final output \( x_0 \) (e.g., log-likelihood, reward, alignment)</li>
      </ul>
  
      <p><strong>2. DeepMind's Framing: Noise Space Search</strong></p>
      <p>Optimize over \( x_T \) to maximize final sample quality via the diffusion model:</p>
      <div>
      \[
      x_T^\ast = \arg\max_{x_T} f(\pi(x_T))
      \]
      </div>
      <p>But this assumes a deterministic or injective mapping from \( x_T \to x_0 \), which fails in stochastic denoising models.</p>
  
      <p><strong>3. Proposed Framing: Trajectory Space Search</strong></p>
      <p>Instead, treat the full denoising trajectory \( \pi \) as the search object:</p>
      <div>
      \[
      \pi^\ast = \arg\max_{\pi \in \mathcal{T}} f(\pi)
      \]
      </div>
      <p>Where \( \pi \) is a stochastic process conditioned on \( x_T \), but with learnable control variables at each step.</p>
  
      <p><strong>4. Degrees of Freedom</strong></p>
      <p>Let each transition step \( x_{t-1} \sim p_\theta(x_{t-1} \mid x_t, \epsilon_t) \) include a noise term \( \epsilon_t \). Then, define the trajectory control space:</p>
      <div>
      \[
      \Pi = \{ \epsilon_T, \epsilon_{T-1}, \ldots, \epsilon_1 \}
      \]
      </div>
      <p>Search now occurs over this set of noise realizations or perturbations to the denoising dynamics.</p>
  
      <p><strong>5. Expressivity Through Trajectory Search</strong></p>
      <p>
        In stochastic diffusion models, the outcome distribution is already high-variance due to noise injected at each step. But variance alone doesnâ€™t imply structure or direction. Trajectory space search reframes generation not as random sampling, but as a search problem over entire denoising paths.
      </p>
      <p>
        Let \( f(\pi) \) be a task-aligned score of a trajectory \( \pi \), such as realism, reward, or semantic alignment. Define the range of achievable scores when searching over full noise paths \( \Pi = \{\epsilon_T, \ldots, \epsilon_1\} \) as:
      </p>
      <div>
      \[
      \mathcal{R}(f, \Pi) := \sup_{\pi \in \Pi} f(\pi) - \inf_{\pi \in \Pi} f(\pi)
      \]
      </div>
      <p>
        Compare this to the narrower set of outputs reachable when only varying the initial noise \( x_T \), and letting the reverse process run stochastically:
      </p>
      <div>
      \[
      \mathcal{R}(f, \Pi) \gg \mathcal{R}(f, \mathcal{T}(x_T))
      \]
      </div>
      <p>
        In other words, trajectory space offers a broader and more navigable landscape of possible generations. While both methods sample from the same underlying stochastic process, trajectory search treats inference as a deliberate exploration of denoising paths rather than a one-shot roll from a prior.
      </p>

      <p><strong>Interpretation</strong></p>
      <p>
        This theory reframes generative inference in diffusion models as a structured search over stochastic trajectories. Since randomness accumulates at every step, the final output depends not just on the starting point \( x_T \), but on the full sequence of noise injections that guide the process. By optimizing over these noise trajectories, we expose a richer set of candidate generations, enabling more expressive and reliable sampling without changing the model itself. This trades off increased compute for better inference, not through control, but through deeper search.
      </p>
  </div>
</body>
</html>