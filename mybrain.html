<!DOCTYPE html>
<html lang="en">
<head>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="UTF-8">
  <title>My Brain — Niel Ok</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      background: #fcfcfc;
      color: #2c2c2c;
      font-size: 14px;
      line-height: 1.6;
      padding: 3rem 1.5rem;
      max-width: 680px;
      margin: 0 auto;
    }
    a {
      color: inherit;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    .back-link {
      font-size: 13px;
      margin-bottom: 2rem;
      display: inline-block;
      color: #777;
    }
    h1 {
      font-size: 1.6rem;
      margin-bottom: 1rem;
    }
    .entry {
      margin-bottom: 2.5rem;
    }
    .entry-date {
      font-size: 12px;
      color: #999;
    }
    .entry-category {
      font-size: 12px;
      font-weight: 500;
      color: #888;
      text-transform: uppercase;
      margin-top: 0.1rem;
      margin-bottom: 0.5rem;
      letter-spacing: 0.04em;
    }
    .entry-content {
      font-size: 13px;
      color: #444;
    }
  </style>
</head>
<body>
  <a class="back-link" href="index.html">← back</a>
  <h1>my brain</h1>

  <ul style="font-size: 13px; margin-bottom: 2.5rem; padding-left: 1rem;">
    <li><a href="#entry-2025-05-18">The Case for Language-Native World Models</a></li>
    <li><a href="#entry-2025-05-10">Formalization of Minimal Latent Representation Learning</a></li>
    <li><a href="#entry-2025-05-09">Intelligence as Compressed Search</a></li>
    <li><a href="#entry-2025-05-03">Feedback-Efficient Intelligence Addendum</a></li>
    <li><a href="#entry-2025-05-02">Formalization of Trajectory Space Search</a></li>
    <li><a href="#entry-2025-04-30">Formalization of Feedback-Efficient Intelligence</a></li>
    <li><a href="#entry-2025-04-22">My Workspace</a></li>
    <li><a href="#entry-2025-04-21">Recursive Superintelligence (Singularity) is Constrained by Drift</a></li>
    <li><a href="#entry-2025-04-09">Reasoning vs Thinking</a></li>
    <li><a href="#entry-2025-04-07">Inference Scaling for Stochastic Diffusion</a></li>
    <li><a href="#entry-2025-01-02">Diffusion and Vectorization</a></li>
    <li><a href="#entry-2024-12-28">When Evolution Stalls, Information Transfer Must Scale</a></li>
  </ul>

  <!-- Entries -->
  <div class="entry" id="entry-2025-05-18">
    <div class="entry-date">May 18, 2025</div>
    <div class="entry-category">world models</div>
    <h2>The Case for Language-Native World Models</h2>
    <div class="entry-content">
      This post is about a framework I’ve been working on that I call “language-native world modeling.” The key idea is to separate language from generation. Language isn’t the output, it’s the interface. What we really want to build is a system that interprets language into structured latent representations of the world, simulates how that world might evolve, and then uses language only when it needs to communicate something back to us.

      It starts with a simple observation: humans reasoned before we spoke. Long before syntax or grammar emerged, our brains were already simulating spatial layouts, predicting actions, modeling goals. Language didn’t give us reasoning. It did, however, give us a powerful way to interface with it. Language supercharged cognition by giving us tools to name abstractions, compare counterfactuals, and share mental simulations. But the underlying substrate of world modeling was already there.

      That’s what I think we’re missing in LLMs. We treat them like they are intelligent because they complete text well. But completing text is just a visible trace of reasoning. It’s bulky. If you try to represent world states through strings alone, you end up needing five sentences to describe something that could be a clean latent vector — a robot in a kitchen, a cup on the floor, a goal to move it. It’s like trying to draw a diagram using only adjectives. The result is cluttered, slow, and semantically brittle.

      That said, there’s growing evidence that language models do form internal representations of the world. Even when trained only on text, they appear to acquire structured latent knowledge about space, causality, physical interactions, and goals. These internal world models seem to support surprisingly coherent behavior, but they’re buried inside the model, inaccessible except through clever prompting or behavioral probing.

      The core problem is this: we don’t have direct control over those world models. We can only interact with them through the lens of text generation. There’s no clean mechanism to extract, simulate, or manipulate the underlying structure, no way to treat the world model as an explicit, evolving state.

      So here’s what I propose. We build a system with three modules:
      <ol>
        <li>
          A semantic encoder that takes natural language and turns it into a structured latent state. This state is a vector, but not a black-box embedding. It’s structured, as in each dimension corresponds to something interpretable, like object properties or agent location. Optionally, we can use an encoder to project this to a learned latent space to make better embeddings for world prediction, almost like a word embedding model, but used instead to embed world states.
        </li>
        <li>
          A latent dynamics model that predicts how this world evolves over time under hypothetical actions. No tokens involved. Just autoregressive simulation in meaning space.
        </li>
        <li>
          A verifier that checks whether the final state matches the desired goal, also given in natural language, but embedded into the same latent space as the world state.
        </li>
      </ol>

      This system is language-native, but token-free at its core. It doesn’t think in words. It thinks in semantic trajectories. When you say, “The robot picks up the cup and places it on the counter,” it builds a latent configuration of the world where the cup moves from floor to counter. And it can run that simulation forward, backward, or in massive parallel - not by predicting strings, but by predicting how the world changes.

      I think this matters for a few reasons:
      <ul>
        <li>First, it lets us reason over meaning instead of syntax. The system doesn’t get distracted by phrasing. It thinks in structure.</li>
        <li>Second, it allows us to separate grounding from generation. We can evaluate, plan, and simulate in latent space, and only decode to language when needed.</li>
        <li>Third, it reframes intelligence as predictive compression over world-state entropy, not token entropy. That gives us a better measure of understanding.</li>
        <li>Fourth, and maybe most importantly: it gives us <em>explicit control</em> over the world model. Not just prompt-hacking. Not just steering with soft constraints. But actual simulation over latent variables we define and interpret.</li>
      </ul>

      I’m not claiming I've already built this system, or even come close. This is a blueprint. But it points to a class of models that feel closer to how we think: not just reacting to words, but actively simulating environments based on them.

      To me, this is what intelligence looks like. Not the ability to finish a sentence. The ability to simulate a world, manipulate it mentally, and test whether your imagined trajectory gets you to the outcome you want.

      Language boosted that power. It gave us a keyboard for simulation. But simulation itself, world modeling, was always the substrate. My claim is that we can go back to that substrate, build from it, and use language as the bridge, not the destination. Read the paper linked on my page for more details on the architecture and how it works.
    </div>
  </div>
  

  <div class="entry" id="entry-2025-05-10">
    <div class="entry-date">May 10, 2025</div>
    <div class="entry-category">compressed representations</div>
    <h2>Formalization of Minimal Latent Representation Learning</h2>
    <div class="entry-content">
      <p>
        This entry formalizes a key idea extending from my compressed search framework: that a model's internal latent space \( \mathcal{Z} \subset \mathbb{R}^D \) is typically larger than necessary for the behaviors it expresses during inference. If we can isolate the empirically used subspace \( \tilde{\mathcal{Z}} \subset \mathcal{Z} \), then it may be possible to learn a new model that operates in a minimally sufficient latent space \( \mathcal{Z}_{\min} \subset \mathbb{R}^d \), where \( d \ll D \), achieving equivalent function with reduced inference cost. Retroactively, I realize that most of this is a formalization for model quantization, but I think it has broader implications for understanding the nature of intelligence and the limits of model compression, especially the latter parts on recursive intelligence and the semantic compressibility limit.
      </p>
  
      <p><strong>1. Latent Usage Estimation</strong></p>
      <p>
        Let \( \text{Enc}: \mathcal{X} \to \mathcal{Z} \) be the encoder for some model \( f: \mathcal{Z} \to \mathcal{Y} \), and let:
      </p>
      <div>
        \[
        \tilde{\mathcal{Z}} = \{ z_i = \text{Enc}(x_i) \mid x_i \in \mathcal{D}_{\text{inference}} \}
        \]
      </div>
      <p>
        be the latent codes actually used during inference on task-relevant data.
      </p>
  
      <p><strong>2. Compression Objective</strong></p>
      <p>
        The goal is to find a smooth embedding \( g: \mathbb{R}^d \to \mathcal{Z} \) such that:
      </p>
      <div>
        \[
        \forall z \in \tilde{\mathcal{Z}},\quad \exists u \in \mathbb{R}^d \text{ s.t. } g(u) \approx z
        \]
      </div>
      <p>
        This defines a compressed latent space \( \mathcal{Z}_{\min} = \mathbb{R}^d \) that preserves semantic variability. \( g \) can be learned using PCA (linear), autoencoders (nonlinear), or manifold learning methods.
      </p>
  
      <p><strong>3. Functional Equivalence</strong></p>
      <p>
        Train a function \( \hat{f}: \mathbb{R}^d \to \mathcal{Y} \) such that:
      </p>
      <div>
        \[
        \hat{f}(u) \approx f(g(u)) \quad \text{for } u \sim \mathcal{Z}_{\min}
        \]
      </div>
      <p>
        This is a distillation process over the compressed latent manifold. The new model \( \hat{f} \) is smaller, faster, and potentially more robust due to reduced sensitivity to unused latent dimensions.
      </p>
  
      <p><strong>4. Intelligence Ratio Comparison</strong></p>
      <p>
        We utilize the idea of intelligence as compressed search described in my previous entry. Let \( T_{\text{brute}} \) be the compute cost of exhaustive search over \( \mathcal{S} \), and \( T_f \), \( T_{\hat{f}} \) be the cost of inference with \( f \) and \( \hat{f} \), respectively. Then define the intelligence of a system as:
      </p>
      <div>
        \[
        \mathcal{I}(f) = \frac{T_{\text{brute}}}{T_f}, \quad \mathcal{I}(\hat{f}) = \frac{T_{\text{brute}}}{T_{\hat{f}}}
        \]
      </div>
      <p>
        If \( \hat{f} \) faithfully replicates \( f \) in \( \tilde{\mathcal{Z}} \), then:
      </p>
      <div>
        \[
        \mathcal{I}(\hat{f}) > \mathcal{I}(f)
        \]
      </div>
      <p>
        This expresses the idea that latent compression improves intelligence not by increasing capability, but by reducing the compute required to reach equivalent outcomes.
      </p>
  
      <p><strong>5. Recursive Intelligence</strong></p>
      <p>
        I suspect that, even after identifying a minimal latent space \( \mathcal{Z}_{\min} \) sufficient to reproduce the original model's outputs, the process of learning \( \hat{f}: \mathcal{Z}_{\min} \to \mathcal{Y} \) may itself reveal internal compressibility within that space. That is, the model might only rely on a structured subregion \( \tilde{\mathcal{Z}}_{\min} \subset \mathcal{Z}_{\min} \) during actual inference.
      </p>
      
      <p>
        This suggests that minimal representations are not necessarily atomic — they may still contain compressible structure, which can be discovered only through functional learning. Formally:
      </p>
      
      <div>
        \[
        \exists\, \tilde{\mathcal{Z}}_{\min} \subset \mathcal{Z}_{\min} \text{ such that } \hat{f}(u) \approx \hat{f}(P(u)) \quad \forall u \in \mathcal{Z}_{\min}
        \]
      </div>
      
      <p>
        where \( P: \mathcal{Z}_{\min} \to \tilde{\mathcal{Z}}_{\min} \) is a projection or learned compression map.
      </p>
      
      <p>
        This leads naturally to a recursive refinement process:
      </p>
      
      <div>
        \[
        \mathcal{Z}^{(0)} \supset \tilde{\mathcal{Z}}^{(0)} \xrightarrow{P_0} \mathcal{Z}^{(1)} \supset \tilde{\mathcal{Z}}^{(1)} \xrightarrow{P_1} \mathcal{Z}^{(2)} \supset \cdots
        \]
      </div>
      
      <p>
        Each \( \mathcal{Z}^{(i)} \) is a latent space learned to be sufficient for the compressed structure of the previous space — not by static reduction, but via introspective learning of functional invariance. This defines a nested compression hierarchy:
      </p>
      
      <div>
        \[
        \hat{f}^{(i)}: \mathcal{Z}^{(i)} \to \mathcal{Y}, \quad \hat{f}^{(i)}(u) \approx \hat{f}^{(i-1)}(g^{(i)}(u))
        \]
      </div>
      
      <p>
        where \( g^{(i)} \) maps upward into the previous latent structure. The process continues until either:
        <ul>
          <li>Inference cost stops decreasing, or</li>
          <li>Further compression degrades semantic capacity</li>
        </ul>
      </p>
      
      <p>
        Intelligence, in this view, is not just the compression of representations — it is the ability to discover recursively compressible structure hidden even within spaces already believed to be minimal.
      </p>

      <p><strong>6. Semantic Compressibility Limit (Principle)</strong></p>

        <p>
        I predict that there exists a lower bound on how much a model’s internal representation space can be compressed while still preserving its ability to express task-relevant distinctions. This bound is determined not by the model, but by the semantic complexity of the task itself.
        </p>

        <div>
        \[
        \exists\, d^\ast \in \mathbb{N} \quad \text{such that} \quad \forall\, d < d^\ast,\, \hat{f}: \mathbb{R}^d \to \mathcal{Y} \text{ cannot preserve } f\text{'s task fidelity.}
        \]
        </div>

        <p>
        Here, \( d^\ast \) is the semantic compressibility limit of the task — the minimal latent dimensionality required for any model to resolve the distinctions that the task demands. Compression beyond this point leads to loss of essential information.
        </p>

        <p>
        Thus, intelligence is not the ability to infinitely compress. It is the ability to approach this limit — to identify and operate at the edge of irreducible complexity. Recursive compression converges not to zero, but to this task-specific boundary.
        </p>
  
      <p><strong>7. Interpretation</strong></p>
      <p>
        This theory reframes model compression as a general strategy for intelligence amplification. Rather than pruning weights or reducing parameters arbitrarily, it proposes targeting the actual semantic manifold that inference operates on. Intelligence emerges not from modeling more — but from modeling just enough, and modeling it efficiently.
      </p>
    </div>
  </div>

  <div class="entry" id="entry-2025-05-09">
    <div class="entry-date">May 9, 2025</div>
    <div class="entry-category">compressed search</div>
    <h2>Intelligence as Compressed Search</h2>
    <div class="entry-content">
      <p>
        In this entry, I offer a view of intelligence at a slightly different angle. It's inspired by my work from Corvus and my work on variance-aware trajectory search inference scaling for robotics action planning using diffusion policy. From these places, I've gained an insight: you can empirically map how a learned model compresses its internal state space. There are two levels to this. First, the model compresses high-dimensional environmental state space into a more compact internal representation space. Second, empirically, the model also learns to further compress that internal state space by only traversing a small, structured subset of it during inference. This dual compression reveals something interesting: intelligent systems don't just learn to model the world, they learn to compress it. From this perspective, intelligent systems may exist to reduce the effective compute required to search through large possibility spaces. It's not just optimization — it's optimization with amortized cost. In this view, intelligence is a compression engine for intractable search. This search can be over actions, thoughts, representations, optimal search methods (at the meta-cognition level), or any other space of possible outcomes, in relation to some objective function. At Corvus, we see this with our systems when we're trying to find the best estimate for your location. The smaller the search space, the more granular and therefore accurate the estimate. All of these spaces are potentially infinite, and would be very expensive to brute-force search.
      </p>
  
      <p><strong>1. Definitions</strong></p>
      <ul>
        <li>\( \mathcal{S} \): a space of possible actions, thoughts, or representations</li>
        <li>\( T_{\text{brute}} \): expected compute cost of brute-force search in \( \mathcal{S} \)</li>
        <li>\( T_{\text{intel}} \): expected compute cost using an intelligent system</li>
        <li>\( \mathcal{I} \): intelligence as the compression ratio of search compute</li>
      </ul>
  
      <div>
        \[
        \mathcal{I} = \frac{T_{\text{brute}}}{T_{\text{intel}}}
        \]
      </div>
  
      <p>
        Higher \( \mathcal{I} \) implies greater intelligence: the system achieves the same or better outcome with fewer evaluations or decisions. This metric is task-general and architecture-independent.
      </p>
  
      <p><strong>2. Interpretation</strong></p>
      <p>
        This theory reframes intelligence as a search-accelerating process that distills the structure of a domain to avoid wasting compute on dead-ends. In biological systems, this manifests as perception, memory, and predictive modeling. In artificial systems, it manifests as amortized inference, inductive biases, or learned heuristics.
      </p>
  
      <p>
        Importantly, \( \mathcal{I} \) is not fixed. A system can learn to improve its own \( T_{\text{intel}} \) over time, i.e., it can compress search even further as it gains experience. This recursive efficiency is a possible axis for defining higher-order intelligence.
      </p>

      <p>
        Another important insight is that, in this view, thinking is about using compute to search more of the compressed space offered by learned models. 
      </p>
  
      <p>
        Intelligence, in this sense, is not the raw power to search everything — it is the ability to know what not to search.
      </p>
    </div>
  </div>

  <div class="entry" id="entry-2025-05-03">
    <div class="entry-date">May 3, 2025</div>
    <div class="entry-category">feedback-efficient intelligence</div>
    <h2>Feedback-Efficient Intelligence Addendum</h2>
    <div class="entry-content">
      <p>This post extends my formalization of feedback-efficient intelligence with a second-level insight: that the structure of the feedback channel itself must be learned or evolved. In other words, agents must discover not just how to update from feedback, but which feedback matters at all.</p>
      <p><strong>1. Definitions</strong></p>
      <ul>
        <li><strong>\( \mathcal{E} \)</strong>: the environment (a stochastic dynamical system)</li>
        <li><strong>\( \pi \)</strong>: the agent’s policy</li>
        <li><strong>\( \hat{M}_t \)</strong>: the internal model of the environment at time \( t \)</li>
        <li><strong>\( \mathcal{F}_t \)</strong>: feedback received at time \( t \)</li>
        <li><strong>\( U_t \)</strong>: utility or performance at time \( t \)</li>
        <li><strong>\( I(\mathcal{F}_t; \Delta \hat{M}_t) \)</strong>: mutual information between feedback and model update</li>
        <li><strong>\( I(\mathcal{E}; \mathcal{F}_t) \)</strong>: mutual information between the environment and the feedback</li>
      </ul>
      <p><strong>2. Meta-Theorem: Feedback Channel Learning</strong></p>
      <p>No intelligent system can be feedback-efficient without either possessing or learning a model of the mutual information structure between the environment and its feedback signals:</p>
      <div>
        \[I(\mathcal{E}; \mathcal{F}_t) > 0 \quad \text{must be learned or evolved.} \]
      </div>
      <div>
        Long-term feedback efficiency requires modeling the feedback channel itself.
      </div>
      <p><strong>3. Interpretation</strong></p>
      <p>This reflects what evolution did for humans: it sculpted our attention, pain, curiosity, and perception toward signals with consistently high mutual information about task success. It’s not just that we learn from feedback. We evolved to learn what counts as feedback.</p>
      <p>This meta-level principle applies to artificial agents as well. In order to sustain feedback-efficient intelligence, they must eventually learn a generative model over the feedback channel. Otherwise, they risk optimizing over noise or reacting to uninformative stimuli. Feedback channels are not given — they are discovered.</p>
    </div>
  </div>

  <div class="entry", id="entry-2025-05-02">
    <div class="entry-date">May 2, 2025</div>
    <div class="entry-category">trajectory space search</div>
    <h2>Formalization of Trajectory Space Search</h2>
    <div class="entry-content">
      This post formalizes the theory I have been calling trajectory space search in stochastic diffusion models. The key idea is to shift the search domain from noise space to trajectory space as we move from deterministic diffusers (under DeepMind's framework) to stochastic ones, potentially enabling more expressive inference.
  
      <p><strong>1. Definitions</strong></p>
      <ul>
        <li>\( \mathcal{X} \): data space (e.g., images, text sequences)</li>
        <li>\( x_T \sim p_T(x) \): initial noise drawn from a base Gaussian distribution</li>
        <li>\( x_t \): sample at diffusion step \( t \) (from \( T \) to 0)</li>
        <li>\( \pi = \{x_T, x_{T-1}, \ldots, x_0\} \): a full denoising trajectory</li>
        <li>\( \pi(x_T) \): a trajectory sampled from the default reverse process starting at \( x_T \)</li>
        <li>\( \mathcal{T}(x_T) \): set of all possible trajectories starting at \( x_T \)</li>
        <li>\( f(\pi) \): objective function scoring the final output \( x_0 \) (e.g., log-likelihood, reward, alignment)</li>
      </ul>
  
      <p><strong>2. DeepMind's Framing: Noise Space Search</strong></p>
      <p>Optimize over \( x_T \) to maximize final sample quality via the diffusion model:</p>
      <div>
      \[
      x_T^\ast = \arg\max_{x_T} f(\pi(x_T))
      \]
      </div>
      <p>But this assumes a deterministic or injective mapping from \( x_T \to x_0 \), which fails in stochastic denoising models.</p>
  
      <p><strong>3. Proposed Framing: Trajectory Space Search</strong></p>
      <p>Instead, treat the full denoising trajectory \( \pi \) as the search object:</p>
      <div>
      \[
      \pi^\ast = \arg\max_{\pi \in \mathcal{T}} f(\pi)
      \]
      </div>
      <p>Where \( \pi \) is a stochastic process conditioned on \( x_T \), but with learnable control variables at each step.</p>
  
      <p><strong>4. Degrees of Freedom</strong></p>
      <p>Let each transition step \( x_{t-1} \sim p_\theta(x_{t-1} \mid x_t, \epsilon_t) \) include a noise term \( \epsilon_t \). Then, define the trajectory control space:</p>
      <div>
      \[
      \Pi = \{ \epsilon_T, \epsilon_{T-1}, \ldots, \epsilon_1 \}
      \]
      </div>
      <p>Search now occurs over this set of noise realizations or perturbations to the denoising dynamics.</p>
  
      <p><strong>5. Expressivity Through Trajectory Search</strong></p>
      <p>
        In stochastic diffusion models, the outcome distribution is already high-variance due to noise injected at each step. But variance alone doesn’t imply structure or direction. Trajectory space search reframes generation not as random sampling, but as a search problem over entire denoising paths.
      </p>
      <p>
        Let \( f(\pi) \) be a task-aligned score of a trajectory \( \pi \), such as realism, reward, or semantic alignment. Define the range of achievable scores when searching over full noise paths \( \Pi = \{\epsilon_T, \ldots, \epsilon_1\} \) as:
      </p>
      <div>
      \[
      \mathcal{R}(f, \Pi) := \sup_{\pi \in \Pi} f(\pi) - \inf_{\pi \in \Pi} f(\pi)
      \]
      </div>
      <p>
        Compare this to the narrower set of outputs reachable when only varying the initial noise \( x_T \), and letting the reverse process run stochastically:
      </p>
      <div>
      \[
      \mathcal{R}(f, \Pi) \gg \mathcal{R}(f, \mathcal{T}(x_T))
      \]
      </div>
      <p>
        In other words, trajectory space offers a broader and more navigable landscape of possible generations. While both methods sample from the same underlying stochastic process, trajectory search treats inference as a deliberate exploration of denoising paths rather than a one-shot roll from a prior.
      </p>

      <p><strong>Interpretation</strong></p>
      <p>
        This theory reframes generative inference in diffusion models as a structured search over stochastic trajectories. Since randomness accumulates at every step, the final output depends not just on the starting point \( x_T \), but on the full sequence of noise injections that guide the process. By optimizing over these noise trajectories, we expose a richer set of candidate generations, enabling more expressive and reliable sampling without changing the model itself. This trades off increased compute for better inference, not through control, but through deeper search.
      </p>
  </div>

  <div class="entry", id="entry-2025-04-30">
    <div class="entry-date">April 30, 2025</div>
    <div class="entry-category">feedback-efficient intelligence</div>
    <h2>Formalization of Feedback-Efficient Intelligence</h2>
    <div class="entry-content">
      This is my first attempt to formalize the ideas in my blog plost on April 21, 2025. I'm not super mathy, but this is the gist of it.
      <p><strong>1. Definitions</strong></p>
      <ul>
        <li>\( E \): the environment (a partially observable stochastic dynamical system)</li>
        <li>\( \pi \): the agent’s policy mapping histories to actions, or put more simply, the agent</li>
        <li>\( \hat{M}_t \): the agent’s learned internal latent model of \( E \) at time \( t \)</li>
        <li>\( \delta_t \): drift error of \( \hat{M}_t \), i.e., true and internal prediction distributions</li>
        <li>\( F_t \): feedback from the environment at time \( t \)</li>
        <li>\( U_t \): utility or task performance at time \( t \)</li>
        <li>\( I_t = I(F_t ; \Delta \hat{M}_t) \): mutual information between feedback and model update</li>
        <li>\( \eta_{t} = \frac{I_t}{\Delta U_t} \): feedback efficiency—feedback per unit performance gain at time \( t \)</li>
      </ul>
  
      <p><strong>2. Axiom: Drift in Internal Representations</strong></p>
      <p><em>Unbounded Drift Axiom:</em></p>
      <p>If an internal model is not continually recalibrated with external feedback, then:</p>
      <p>\[
      \lim_{t \to \infty} \delta_t = \infty
      \]</p>
      <p>(under finite precision and chaotic/stochastic environments)</p>
  
      <p><strong>3. Theorem: No Self-Sufficient Intelligence</strong></p>
      <p><em>Nonzero Feedback Theorem:</em></p>
      <p>No intelligent system can maintain bounded error \( \delta_t < \varepsilon \) without cumulative feedback:</p>
      <p>\[
      \int_0^\infty \|F_t\| \, dt > 0 \quad \forall \varepsilon > 0
      \]</p>
  
      <p><strong>4. Intelligence as Feedback Efficiency</strong></p>
      <p>Define feedback needed per unit of utility improvement:</p>

      <p>
      \[
      \eta_{t}(\pi) = \limsup_{t \to \infty} \frac{I(F_t ; \Delta \hat{M}_t)}{\Delta U_t}
      \]
      </p>  

      <p>Then agent \( \pi \) is more intelligent than agent \( \pi' \) over task class \( T \) at time \( t \) if:</p>

      <div>
      \[
      \eta_{t}^{T}(\pi) < \eta_{t}^{T}(\pi')
      \]
      </div>
  
      <p><strong>5. Corollary: The Singularity is Asymptotic</strong></p>
      <p>
      Even self-improving superintelligences cannot reach perfect intelligence (i.e., \( \eta = 0 \)) due to irreducible environmental entropy:
      </p>

      <div>
      \[
      \forall \pi \in \mathcal{A}_{\text{phys}}, \quad \forall t, \quad \eta_{t}(\pi) > 0
      \]
      </div>

      <p>
      Here, \( \mathcal{A}_{\text{phys}} \) denotes the set of all physically realizable agents, that is, agents embedded in environments with bounded memory, finite energy, and nonzero entropy.
      </p>
  
      <p><strong>6. Model Compression View</strong></p>

      <ul>
        <li>\( D_{\mathrm{KL}}(P_{E_{t}} \| \hat{P_{t}}) \): divergence between true and internal prediction distributions at time \( t \), where \( \hat{P_{t}} \) is the internal model at time \( t \) and \( P_{E_{t}} \) is the true probability distribution at time \( t \)</li>
      </ul>

      <p>
      Define a new quantity \( \xi_{t} \), which captures the agent’s compression inefficiency. That is, how much model error remains per bit of feedback about the model at time \( t \):
      </p>

      <div>
      \[
      \xi_{t} = \frac{D_{\mathrm{KL}}(P_{E_{t}} \| \hat{P_{t}})}{I(F_t ; \hat{M_{t}})}
      \]
      </div>

      <p>
      In contrast to \( \eta_{t} \), which quantifies feedback cost per performance gain, \( \xi_{t} \) quantifies model accuracy per bit of feedback. Both reflect intelligence in different regimes: outer utility vs inner alignment.
      </p>
  
      <p><strong>Interpretation</strong></p>
      <p>
        This reframes intelligence not as raw computational power or self-sufficiency, but as the ability to compress reality—updating latent models with minimal feedback. The lower the feedback efficiency \( \eta_{t} \), the more intelligent the agent in a task-general sense. The lower the compression inefficiency \( \xi_{t} \), the more precisely it internalizes external structure. In both views, intelligence is fundamentally defined by how efficiently error is reduced per bit of external signal.
      </p>
    </div>
  </div>
  
  <div class="entry", id="entry-2025-04-22">
    <div class="entry-date">April 22, 2025</div>
    <div class="entry-category">my workspace</div>
    <h2>My Workspace</h2>
    <div class="entry-content">
      <img src="images/my_workspace_ai.png" alt="Niel's workspace according to AI" style="width:100%; max-width:100%; margin-bottom: 1rem; border-radius: 4px;">
      My workspace according to AI.
    </div>
  </div>

  <div class="entry", id="entry-2025-04-21">
    <div class="entry-date">April 21, 2025</div>
    <div class="entry-category">recursive superintelligence</div>
    <h2>Recursive Superintelligence (Singularity) is Constrained by Drift</h2>
    <div class="entry-content">
      At Brains in Silicon, Stanford’s neuromorphic computing lab, I work in neuroscience theory, hoping to find inspiration for the hardware and algorithms people to work with. Recently, I’ve been messing with place fields, grid cells, and BTSP (behavioral time scale plasticity, a novel proposal for a learning mechanism that occurs on slower time-scales), mainly to see how animals localize themselves in unfamiliar environments. In my research, I’ve found that, while the brain can guess where the animal is located based on internal estimates of motion (like an inertial measurement unit), this estimate drifts significantly without external stimuli. This coincides with observations I’ve had while building Corvus, where I’ve learned that it is impossible to build a system that is able to navigate perfectly with no external input, even with an inertial navigation system. This brings me to an insight: all forms of intelligence, no matter how sophisticated, rely on input from the environment for feedback on actions, including metacognitive updates. Why? Because intelligence is all about how well an agent is able to do in its environment. No agent can internally verify the success of an action at 100% accuracy. Perfect simulation and pre-verification of action outcomes is physically intractable due to the inherent stochasticity and chaos present in real-world systems, including quantum uncertainty. To speed up the rate of updates, agents can build latent representations (internal models) of the world to simulate actions and feedback, but these representations are approximate, and from what I’ve learned with Corvus and neuroscience, all systems that are not 100% accurate have drift that grows exponentially over large time-scales. External input is required to recalibrate these internal representations. Thus, even a self-improving superintelligence will depend upon external input for feedback. So, what would set apart this superintelligence from other, normal intelligences? One possible axis would be the magnitude of environmental input required per intelligence update, where the intelligence update could be across various tasks such as math, maze solving, language, etc. I would say that, generally, the more intelligent an agent is, the closer to 0 it is on this axis across many different tasks. If there were a self-improving agent, it would attempt to get closer to 0 on this axis across many tasks (note: if my theory about drift is true, it is impossible to have a perfect singularity for intelligence, since 0 cannot be reached, although something very close to one might be possible). How would it get close to 0? Like I hinted at earlier, by improving its latent representations, it can get close to 0. Then, a well-designed, self-improving superintelligent agent would be very good at creating its own latent representations of the world. This reframes superintelligence not as the ability to recursively rewrite itself in isolation, but as the capacity to build highly accurate internal representations of the world that minimize, but never eliminate, the need for feedback. Given sufficient compute, these internal representations can be updated and queried at high speed, shifting the bottleneck from processing time to feedback efficiency. Intelligence is thus better understood not as raw computational power, but as the ability to form accurate models with minimal reliance on external correction: a property of a feedback-efficient loop between agent and world. 
    </div>
  </div>

  <div class="entry", id="entry-2025-04-09">
    <div class="entry-date">April 9, 2025</div>
    <div class="entry-category">intelligence, scaling thinking, reasoning</div>
    <h2>Reasoning vs Thinking</h2>
    <div class="entry-content">
      Looking at all of the recent work attempting to scale reasoning for language models with variations of Chain-of-Thought (DeepSeek-R1, OpenAI o-1, o-3), plus with some reflection on how human learning and thinking works, I have some insights. It seems to me that reasoning and thinking for language models should be established as two different things. Reasoning, to me, seems like a cognitive action that utilizes facts and step-by-step logic to get from one point to another. Basically, CoT. When we talk about reasoning for language models, we think about how we can bake reasoning into the models with human text that demonstrates reasoning. Humans can learn how to reason, and so can models. Meanwhile, when it comes to thinking, it seems like it is more about taking an existing model, no matter how good it is at reasoning, and allowing it to search a larger portion of the space of possible answers before narrowing it down to the most ideal one. When people say someone is thinking hard, it means that person is using a lot of internal compute on a certain problem, not that the person is reasoning particularly well or anything like that. DeepMind’s inference scaling framework for deterministic diffusion models is an example of a framework for thinking. Thinking can be more quantitative, without the need for the abstraction of human language on top of the numbers that the model is actually running. I think reasoning sacrifices mathematical interpretability in favor of qualitative interpretability, while thinking sacrifices qualitative interpretability in favor of mathematical interpretability. So, reasoning looks more ethical while thinking looks more promising in terms of creating superintelligent, flexible models. Now, I believe that we can eventually build tools for interpreting thinking, so I am more interested in thinking than reasoning. I work on structured metacognition frameworks and denoising trajectory search for text, image, and robotics policy diffusion models, because that’s thinking. These projects are attempts to make thinking controllable. Thinking speaks more in the language of the model than the human, and it doesn’t restrict the model to our mode of communication. I think reasoning may force the model to commit to certain trajectories too early, while with thinking, you can basically control how long the model explores and when it should commit. The framework of thinking just gives you more control over the model in my opinion. I’m working on building more of a theoretical framework on this in my free time, which I’m excited about.  
    </div>
  </div>

  <div class="entry", id="entry-2025-04-07">
    <div class="entry-date">April 7, 2025</div>
    <div class="entry-category">diffusion models, llms, inference scaling</div>
    <h2>Diffusion Models and Inference Scaling</h2>
    <div class="entry-content">
      Over the past few months, I’ve been diving into a research direction that takes inference-time compute scaling for diffusion models somewhere fundamentally different from where DeepMind has gone. Their approach focuses on deterministic diffusion models and treats inference-time scaling as a search over the initial noise vector, as if the entire generative process hinges on getting that first random sample just right. But that framing doesn’t hold up when you step into the stochastic regime. In stochastic diffusion models, randomness isn’t a one-time deal at the start. It gets injected at every step of the denoising process. The result? The final sample isn’t just determined by where you start, it’s shaped by the entire trajectory the model takes through denoising space. So optimizing only over initial noise is like trying to steer a rocket by tweaking the launch angle and ignoring all mid-course corrections. What I’ve been exploring is this: inference-time compute scaling for stochastic diffusion models shouldn’t be framed as a search over initial noise vectors. Instead, it should be framed as a search over denoising trajectories. That shift in perspective changes everything. It acknowledges the true degrees of freedom in the system. It’s harder computationally, but the payoff is real. Instead of being locked into a narrow slice of the outcome space, trajectory-based search opens up far more expressive possibilities. Multiple trajectories can land you in semantically different, yet equally plausible regions of the output space. That makes it not just more powerful, but more useful, especially when you care about conditioning, controllability, or aligning with task-specific goals. This has led to a few ideas I want to push further: One, a formal comparison between noise space and trajectory space. What’s actually gained in terms of dimensionality, expressiveness, or alignment capability? Two, could we design and demonstrate trajectory-space search algorithms, even in simplified settings, that outperform initial-noise search with less proportional coverage? That would be a strong empirical signal. Three, maybe the most ambitious one: if trajectory space really is this rich, it might be possible to prove that inference in diffusion models, especially for text, is more scalable than autoregressive LLM inference. 
    </div>
  </div>  

  <div class="entry", id="entry-2025-01-02">
    <div class="entry-date">January 2, 2025</div>
    <div class="entry-category">ml research, neuroscience</div>
    <h2>Diffusion and Vectorization</h2>
    <div class="entry-content">
      Recently, I’ve become interested in implementing diffusion (image generation) from scratch, including the neural network module used to do it (I’m building myself a mini-PyTorch using only numpy). Because of this, I’ve had the opportunity to reflect a lot on what data should look like when passed to a neural network, and how we can represent different types of information as input for a neural network. For example, for a diffusion model, you somehow have to feed to the neural network both the image and the step number of diffusion you are on in order for the neural network to learn how to denoise step by step. Now, at first, I thought you could just concatenate the step number to the flattened image vector input, but my intuition told me that because the image was so much more high-dimensional than the single number step, I must be incorrect. I figured that there must be some step where I turn the single number step into a vector of larger dimension before concatenation so that the step number is more obvious to the neural network. Turns out, there are many ways to do this. This brought me to an insight: as long as some piece of information can be vectorized, it can be the input to a neural network. Then, a lot of ML research must be figuring out the best way to vectorize different pieces of information for a neural network to digest. This is very interesting. For example, one idea I had was to vectorize high-level physics inputs for video-generation models. Maybe a video-generation model could take in a text prompt and a pre-reasoned set of words that best describe the appropriate physics for the imagery to be generated. The way objects move underwater is very different from the air, and so if we trained a model to discriminate between those two explicitly while generating video, perhaps it would be easier to come up with good physics in video models. Anyways, one of my key questions coming away from today is: is it possible to create a framework for testing what method of vectorization maximizes the performance of a neural network at digesting a particular piece of information? Another thought I have that’s an offshoot of the previous one: do biological neurons specialize as a result of starting to process input or are they somewhat primed as a result of evolution, even before they are used to process any signals? Are auditory and visual neurons that way before they are ever used to process sound or light, or do they become that way after receiving a bunch of input in that form? I ask because this would help for me to determine whether learning the most optimal vectorization of information should be part of pre-training, if it should be trained completely separately, or if it should be something in-between.
    </div>
  </div>  

  <div class="entry", id="entry-2024-12-28">
    <div class="entry-date">December 28, 2024</div>
    <div class="entry-category">evolution</div>
    <h2>When Evolution Stalls, Information Transfer Must Scale</h2>
    <div class="entry-content">
      Any improvements to a person’s physique, increase in knowledge, or anything else that a person developed which was helpful to them in their lifetime is not passed down to their children by their genes. All that is passed down is a copy of their own genes, their partner’s genes, and some mutations. What does this mean? My first thought is that maybe this is nature betting that basic traits (fundamental intelligence level, strength level, etc.) of an organism are more valuable than whatever information or strength it could have received from its parent for its survival and eventual reproduction. However, the idea of the selfish gene reframes genes such that they have the desire to be passed on, and therefore optimize the organism for which they are the genes of to be best at passing on the genes. Thus, there would be no additional energy or matter spent on transferring information or traits developed during an organism’s lifetime not relevant to survival and reproduction. This brings me to a question. Is the selfish gene still the most optimal way to advance the human race, and has it ever been? A thought I have about this is that, for the majority of the existence of life, the selfish gene probably also happened to be the most optimal way to advance a species. When survival is difficult to come by and energy low in availability, it makes sense for a species to evolve to be optimal for survival and reproduction. Things like information transfer (as a direct result of reproduction, not just through the parent teaching the offspring) from parent to offspring can be wasteful since there is nowhere near a guarantee that the offspring will also be able to pass down its own information to its offspring. Furthermore, this mechanism would, in most cases, ensure that surviving organisms are the most optimal, and not alive simply because of some random piece of information their ancestors learned that the others did not. Solid fundamental traits must be built first. With computers, it is worth waiting until we have the right fundamental architecture to scale before we actually do scale. Like computers, for organisms, it might be worth waiting until the right traits are in place before scaling up the energy and matter required for reproduction in order to speed up advancement. I think that, at the moment, humans have come to the point where it might be worth scaling. Our architecture shows no signs of changing due to natural selection. We have no shortage of resources compared to the days when we were just trying to survive, and we have transformed the world to suit our tastes. If in evolution, the world transforms the organisms to its taste, the opposite is true now for humans. In this strange scenario, the only thing we can improve now is the efficiency of information transfer between generations. What I am saying is that the next big way for the human race to advance might be to think about how to embed information from a parent in their DNA or some similar mechanism so that their child inherits it.
    </div>
  </div>

</body>
</html>