<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>My Brain — Niel Ok</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      background: #fcfcfc;
      color: #2c2c2c;
      font-size: 14px;
      line-height: 1.6;
      padding: 3rem 1.5rem;
      max-width: 680px;
      margin: 0 auto;
    }

    a {
      color: inherit;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    .back-link {
      font-size: 13px;
      margin-bottom: 2rem;
      display: inline-block;
      color: #777;
    }

    h1 {
      font-size: 1.6rem;
      margin-bottom: 1rem;
    }

    .entry {
      margin-bottom: 2.5rem;
    }

    .entry-date {
      font-size: 12px;
      color: #999;
    }

    .entry-category {
      font-size: 12px;
      font-weight: 500;
      color: #888;
      text-transform: uppercase;
      margin-top: 0.1rem;
      margin-bottom: 0.5rem;
      letter-spacing: 0.04em;
    }

    .entry-content {
      font-size: 13px;
      color: #444;
    }
  </style>
</head>
<body>
  <a class="back-link" href="index.html">← back</a>
  <h1>my brain</h1>

  <div class="entry">
    <div class="entry-date">January 2, 2025</div>
    <div class="entry-category">ml research, neuroscience</div>
    <div class="entry-content">
      Recently, I’ve become interested in implementing diffusion (image generation) from scratch, including the neural network module used to do it (I’m building myself a mini-PyTorch using only numpy). Because of this, I’ve had the opportunity to reflect a lot on what data should look like when passed to a neural network, and how we can represent different types of information as input for a neural network. For example, for a diffusion model, you somehow have to feed to the neural network both the image and the step number of diffusion you are on in order for the neural network to learn how to denoise step by step. Now, at first, I thought you could just concatenate the step number to the flattened image vector input, but my intuition told me that because the image was so much more high-dimensional than the single number step, I must be incorrect. I figured that there must be some step where I turn the single number step into a vector of larger dimension before concatenation so that the step number is more obvious to the neural network. Turns out, there are many ways to do this. This brought me to an insight: as long as some piece of information can be vectorized, it can be the input to a neural network. Then, a lot of ML research must be figuring out the best way to vectorize different pieces of information for a neural network to digest. This is very interesting. For example, one idea I had was to vectorize high-level physics inputs for video-generation models. Maybe a video-generation model could take in a text prompt and a pre-reasoned set of words that best describe the appropriate physics for the imagery to be generated. The way objects move underwater is very different from the air, and so if we trained a model to discriminate between those two explicitly while generating video, perhaps it would be easier to come up with good physics in video models. Anyways, one of my key questions coming away from today is: is it possible to create a framework for testing what method of vectorization maximizes the performance of a neural network at digesting a particular piece of information? Another thought I have that’s an offshoot of the previous one: do biological neurons specialize as a result of starting to process input or are they somewhat primed as a result of evolution, even before they are used to process any signals? Are auditory and visual neurons that way before they are ever used to process sound or light, or do they become that way after receiving a bunch of input in that form? I ask because this would help for me to determine whether learning the most optimal vectorization of information should be part of pre-training, if it should be trained completely separately, or if it should be something in-between.
    </div>
  </div>  

  <div class="entry">
    <div class="entry-date">December 28, 2024</div>
    <div class="entry-category">evolution</div>
    <div class="entry-content">
      Any improvements to a person’s physique, increase in knowledge, or anything else that a person developed which was helpful to them in their lifetime is not passed down to their children by their genes. All that is passed down is a copy of their own genes, their partner’s genes, and some mutations. What does this mean? My first thought is that maybe this is nature betting that basic traits (fundamental intelligence level, strength level, etc.) of an organism are more valuable than whatever information or strength it could have received from its parent for its survival and eventual reproduction. However, the idea of the selfish gene reframes genes such that they have the desire to be passed on, and therefore optimize the organism for which they are the genes of to be best at passing on the genes. Thus, there would be no additional energy or matter spent on transferring information or traits developed during an organism’s lifetime not relevant to survival and reproduction. This brings me to a question. Is the selfish gene still the most optimal way to advance the human race, and has it ever been? A thought I have about this is that, for the majority of the existence of life, the selfish gene probably also happened to be the most optimal way to advance a species. When survival is difficult to come by and energy low in availability, it makes sense for a species to evolve to be optimal for survival and reproduction. Things like information transfer (as a direct result of reproduction, not just through the parent teaching the offspring) from parent to offspring can be wasteful since there is nowhere near a guarantee that the offspring will also be able to pass down its own information to its offspring. Furthermore, this mechanism would, in most cases, ensure that surviving organisms are the most optimal, and not alive simply because of some random piece of information their ancestors learned that the others did not. Solid fundamental traits must be built first. With computers, it is worth waiting until we have the right fundamental architecture to scale before we actually do scale. Like computers, for organisms, it might be worth waiting until the right traits are in place before scaling up the energy and matter required for reproduction in order to speed up advancement. I think that, at the moment, humans have come to the point where it might be worth scaling. Our architecture shows no signs of changing due to natural selection. We have no shortage of resources compared to the days when we were just trying to survive, and we have transformed the world to suit our tastes. If in evolution, the world transforms the organisms to its taste, the opposite is true now for humans. In this strange scenario, the only thing we can improve now is the efficiency of information transfer between generations. What I am saying is that the next big way for the human race to advance might be to think about how to embed information from a parent in their DNA or some similar mechanism so that their child inherits it.
    </div>
  </div>

  <!-- Add more entries below -->

</body>
</html>