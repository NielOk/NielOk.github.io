<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Niel Ok</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="site.css">
</head>
<body>
  <div class="content">
    <div class="top-bar">
      <div class="top-left">
        <a href="/" class="active">home</a>
        <a href="experience/">experience</a>
        <a href="/writing/">writing</a>
        <a href="/reading/">reading</a>
      </div>
    </div>

    <h1>Niel Jongwhan Ok</h1>
    <div class="description">
      systems+theory guy building Null Labs as cofounder/CTO after leaving Stanford.
    </div>

    <h2>About Me</h2>
    <ul class="about-list">
      <li>grew up across Louisiana, Colorado, Texas, Ohio</li>
      <li>got my start in AI in 2018 building diagnostic & generative models for medical imaging</li>
      <li>became fascinated at 15 by quantum entanglement and ER=EPR. wrote a simulator supporting mid-circuit statevector extraction (sponsored by Google Quantum, filled a gap in IBM/Google tools) and used it to develop quantum teleportation protocols for new Google hardware</li>
      <li>studied EE at Stanford</li>
      <li>curiosity about the gap between neural nets and the brain led me into theoretical neuroscience and neuromorphic computing research (Stanford Brains in Silicon), where I modeled dendrite-level dynamics with circuits and differential equations</li>
      <li>spent time between DeepMind and a DeepMind spinout (hire number 2) working on automating AI research and inference scaling/interpretability methods for robotics and text diffusion models</li>
      <li>co-founder/CTO of Null Labs, leveraging computable physics to train autonomous systems</li>
    </ul>

    <h2>Thoughts</h2>
    <ul class="about-list">
      <li>language can structure thought in useful ways but also restricts it</li>
      <li>scaling laws: learning is incremental under today's frameworks</li>
      <li>different frameworks = different slopes. but the right one may escape gradualism</li>
      <li>AI progress has plateaued on internet data. the next frontier will involve harnessing computable physics, structured variation, and interactions with environments more effectively</li>
      <li>a lot of physics consists of abstractions that reduce the interactions of countless particles into tractable computations. some abstractions, however, aren't so computable</li>
      <li>neural nets themselves could serve as functional approximators for complex interactions not considered easily computable in traditional physics</li>
      <li>physics, math, language, and computation all have generative asymmetry. they all exhibit how small sets of rules can lead to rich behaviors. there may be a reason for this structure</li>
      <li>based on the performance of recent AI models, it seems consciousness may not be required to create digital beings that can perform tasks at the level of human intelligence. what exactly is the purpose of consciousness, then?</li>
    </ul>

    <div class="footer">
      <div class="footer-left">
        <div class="social-links">
          <a href="https://github.com/NielOk" target="_blank">github</a>
          <a href="https://twitter.com/nielok20" target="_blank">twitter</a>
          <a href="https://www.linkedin.com/in/niel-ok" target="_blank">linkedin</a>
          <a href="https://huggingface.co/NielOk" target="_blank">hugging face</a>
        </div>
        <div class="emails">
          <div>niel@n0labs.com</div>
          <div>nielok@stanford.edu</div>
        </div>
      </div>
    </div>
  </div>
</body>
</html>
